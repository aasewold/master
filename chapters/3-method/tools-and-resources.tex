\section{Tools and Resources}

Training vision transformers is a computationally heavy task.
To fulfill the computational demands during this research,
we have utilized three main computational resources which will be described in the following sections. To develop on these resources we mainly used the free and open-source code editor Visual Studio Code (VS Code) \cite{vs-code}. VS Code has a large ecosystem of extensions enabling support for debugging, smart completions and more in most programming languages. This includes an extension for connecting to remote machines in the editor using secure shell (SSH) \cite{vs-code-ssh}, a tool we use extensively when working with the project. Note that all of the resources below requires a connection through NTNU's campus network or the NTNU VPN. Finally, to synchronise progress within the team and to keep track of code changes, we decided to create a GitHub organization for storing all relevant project code and scripts.\footnote{\url{https://github.com/orgs/aasewold}} This also made it easy to synchronise code changes between the different resources we used when developing.


\subsection{IDI Horizon}
For initial development and small-scale testing,
we used virtual machines from
IDI Horizon - Virtual Desktop Infrastructure for Visual Computing.
These are equipped with NVIDIA A10-24Q GPUs and CUDA version 11.4.
Access with a desktop graphical interface have been available through VMWare Horizon \cite{software:vmware:horizon}, but we mainly accessed the virtual machines using SSH from a terminal or with VS Code as described above. This is because we did not have much use of the GUI for the experiments in this project, and we rather preferred the lower latency when working directly through SSH.


\subsection{\texttt{nap02}}
While the IDI Horizon virtual machines were great for starting our research, we quickly realized that we would benefit from more performant hardware for longer training sessions and for running the CARLA simulator. We therefore got access to the \texttt{nap02}-server,
a bare-bones computer equipped with
2x Intel Xeon Gold 6342 CPUs,
2x NVIDIA A100 80G GPUs with CUDA version 11.6,
and 512 GiB RAM. This server is shared between members of \acrshort{naplab}, but during the semester we were mostly the only ones using it, meaning we could take advantage of all of the available resources.


\subsection{Idun}
To prepare for a larger amount of experiments during the master's thesis,
we have built tooling and documentation to utilize the Idun High Performance Computing cluster.
Idun provides 864 CPU cores and 80 NVIDIA GPUs to the Department of Computer Science \cite{idun:overview},
and thus enables a much higher degree of parallelization for future experiments.
The cluster is managed using the Slurm Workload Manager \cite{slurm}.


\subsection{VCXR}
For our master's thesis we got access to a new standalone desktop computer at campus called VCXR. This computer is equipped with 
a 24 core Intel i9-13900KF CPU, a NVIDIA GeForce RTX 4090 24G GPU and 32 GiB RAM. It runs Ubuntu 20.04 with CUDA version 12. The main advantage of this machine was to have a physical desktop to use for building CARLA from source, modelling a new vehicle in Blender and importing it into the simulator via Unreal Editor (see \cref{sec:custom-vehicle}), something that was not possible to do on \texttt{nap02} or Idun. It was also used to inspect graphical differences between CARLA versions 0.9.10.1 and 0.9.14 (see TODO).
